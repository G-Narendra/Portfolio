
Name your model
Name your model
As the first step you will need to name your model. Make sure you name your model such that it is both specific and descriptive, as you are likely going to have quite a few models across the season.

My suggestion is using the track or race name and a version number - so for this demonstration, I am going to use SummitSpeedway-V1 (as I am going to be training on that track) and I would then increment the version number with each clone of the model.




Choose track
Choose track
Now you need to select the track to use for training your model.

Note that the more challenging the track, the more training time you will need - and possibly also a more complex reward function.

As a rule of thumb, if you are competing in the Student League then train on the track which the competition is using. In this example, I am training for a race on the Summit Speedway track.



Choose algorithm type
Choose algorithm type
In this step you can choose the reinforcement learning training algorithm to use. This is where things start to get really interesting, as we are now exercising some control over how our model learns.

In this lesson we are just concerned about getting a training job up and running, so choose either PPO or SAC but don’t worry too much about which one you select. In the next lesson we will deep dive into the two algorithms.





Customize reward function
Customize reward function
You now have the opportunity to customize the reward function. This is the piece of code which determines how much the agent should be rewarded for its actions.

There are three sample reward functions available, and we are going to deep dive into these in a future lesson - so for the moment select “Follow the centerline” which will give you a reward function that will reward the agent for staying close to the centreline of the track. This is a great starting reward function which you can build upon later.




Choose duration
Choose duration
Finally, you can configure the options for model training. I suggest you start by doing 60 minutes of training, and give your model a description so you can find it later on (such as a quick summary of the chosen algorithm along with reward function).

If you would like to participate in the Student League make sure you also tick the box to submit to the leaderboard - there’s no harm in doing this, even for a simple model, so give it a shot. You can submit retrained and new models as many times as you like.

When you are ready to start training, select the “Train your model” button.









AWS DeepRacer offers two training algorithms:

Proximal Policy Optimization (PPO)
Soft Actor Critic (SAC)

The first thing to point out is that AWS DeepRacer uses both PPO and SAC algorithms to train stochastic policies. So they are similar in that regard. However, there is a key difference between the two algorithms.

PPO uses “on-policy” learning. This means it learns only from observations made by the current policy exploring the environment - using the most recent and relevant data. Say you are learning to drive a car, on-policy learning would be analogous to you reviewing a video of your most recent lesson and taking note of what you did well, and what needs improvement.

In contrast, SAC uses “off-policy” learning. This means it can use observations made from previous policies exploration of the environment - so it can also use old data. Going back to our learning to drive analogy, this would involve reviewing videos of your driving lessons from the last few weeks. Even though you have probably improved since those lessons, it can still be helpful to watch those videos in order to reinforce good and bad things. It could also include reviewing videos of other drivers to get ideas about good and bad things they might be doing.

So what are some benefits and drawbacks of each approach?

PPO generally needs more data as it has a reasonably narrow view of the world, since it does not consider historical data - only the data in front of it during each policy update. In contrast, SAC does consider historical data so it needs less new data for each policy update.
That said, PPO can produce a more stable model in the short-term as it only considers the most recent, relevant data - compared with SAC which might produce a less stable model in the short-term since it considers less relevant, historical data.
So which should you use? There is no right or wrong answer. SAC and PPO are two algorithms from a field which is constantly evolving and growing. Both have their benefits and either one could work best depending on the circumstance.

As you’ll learn as you continue along your machine learning journey, it involves a lot of experimentation and tuning to see what is going to work best for you.




The reward function
In order to calculate an appropriate reward you need information about the state of the agent and perhaps even the environment. These are provided to you by the AWS DeepRacer system in the form of input parameters - in other words, they are parameters for input into your reward function.

There are over 20 parameters available for use, and the reward function is simply a piece of code which uses the input parameters to do some calculations and then output a number, which is the reward.

The reward function is written in Python as a standard function, but it must be called reward_function with a single parameter - which is a Python dictionary containing all the input parameters provided by the AWS DeepRacer system.






Improving the reward function

Just because you have trained a model doesn't mean you cannot change the reward function. You might find that the model is exhibiting behavior you want to de-incentivize, such as zig-zagging along the track. In this case you may include code to penalize the agent for that behavior.

These reward functions we looked at in this section are just examples, and you should experiment and find one which works well for you. A list of all the different input parameters available to the reward function can be found in the AWS DeepRacer Developer Guide, with full explanations and examples: here 

The reward function can be as simple, or as complex, as you like - just remember, a more complex reward function doesn't necessarily mean better results.



Once you have trained a model using the AWS DeepRacer console, it can be loaded onto an AWS DeepRacer device. The onboard computer on the AWS DeepRacer uses this model to make decisions, a process known as machine learning inference.

Machine learning inference involves running input data through a machine learning model which then outputs a prediction. This prediction is based upon past experience and subsequent knowledge which the model has gained through this training.

In the case of AWS DeepRacer, the machine learning model takes input in the form of images from the cameras, and data from the optional LiDAR sensor. The model then provides output as to what it thinks is the optimal steering and speed actions the car should take to stay on the track.

For example, a well trained model would output that the car has to take a left turn for an image input captured at a left turn on the track - as this would maximize its chance of staying on the track and completing the lap.

The inference process can be compute intensive. In the case of AWS DeepRacer, the inference needs to occur in real-time as the car drives on the track.

We can measure the performance of inference with two metrics:

The "inference rate" which is the number of inferences which can be done per second; and
The "inference time" or "inference latency" which is time taken to run a single inference.
Ultimately, we want to maximize the rate of inference and therefore the number of decisions made every second. However, the rate of inference is dependent on many factors. The most obvious is the performance of the machine running the inference, such as the CPU (Central Processing Unit) speed, whether any GPU (Graphical Processing Unit) acceleration is present, and the amount of system RAM (Random Access Memory) available.

There are also other factors. One is the machine learning framework used for training. There are many different frameworks available, such as TensorFlow, PyTorch, and Apache MXNet. These frameworks provide the tools and services to build machine learning models, including the training algorithms such as PPO (Proximal Policy Optimization) and SAC (Soft Actor Critic), which you might be familiar with from chapter 3.1 Training algorithms in this module.

You don’t need to worry about these frameworks since it’s all taken care of for you with AWS DeepRacer, but it is worthwhile at least having some awareness of their presence and purpose.

Now that we have a better understanding of how the AWS DeepRacer makes decisions, we will look at some of the challenges involved in making this all work in the next section.






When people think of machine learning (ML) models they often imagine them running on large, powerful computers. Some real-world ML applications, like real-time traffic monitoring, need computers powerful enough to provide the high inference rates necessary for complex, high stakes tasks.

It also may be more cost-effective to centralize inferencing in one place. In these kinds of situations the data is fed from edge devices, such as traffic cameras, all across a city to a central server for processing.

However, what happens when sending the data to a central server for inference isn’t a good option and we need to perform inference locally? This is the situation with AWS DeepRacer. It would take too long to send the data to a remote server, perform the inference, and return the result to the AWS DeepRacer car. By that time the vehicle might have already driven off the track! In this situation we need to perform the inference locally on the device. We call this “inference at edge”.

However, this presents a challenge. The compute available on edge devices is restricted. In the case of AWS DeepRacer, it is a relatively small battery powered computer. So, we need to balance the needs of efficient and effective inference while also working within the bounds of the compute available.

In the next section, we are going to look at some general challenges of computing at the edge and learn how to optimize for your device.
So far we have discussed two places where inference can be performed: on large, powerful computers—like a cloud server—or on low-power devices like the AWS DeepRacer device. We call these low-power devices edge devices. Inference on these devices is called real-time inference at edge.

Latency is the time that it takes to receive a result back from a remote computer from the moment the data is sent out from your device. In most cases, inference at the edge results in lower latency because the input doesn’t need to be sent out—It can all be done in one place. This is particularly important for the AWS DeepRacer device which needs to make real-time decisions. However, when you create a deep learning model for the AWS DeepRacer device, you have to consider that edge devices are generally less powerful and have less compute power than what is available in the cloud.

To infer data through a deep learning model we need an inference engine. If you think of the inference engine as the car’s engine then the deep learning model is the fuel. This combination determines how fast and how efficient the car will run. You want to make sure the fuel (your model) is appropriate for the engine (the device) it is running on.

So, we need to do some more work and optimize our trained model so that it performs well when loaded onto the AWS DeepRacer device. In the next section we are going learn how to optimize our model with the OpenVINO toolkit and AWS DeepRacer.

In the previous section we discussed the need to optimize our model for the computer onboard the AWS DeepRacer device so that we can achieve acceptable inference performance. To do this, OpenVINO toolkit is used in conjunction with the AWS DeepRacer device.

OpenVINO stands for Open Visual Inferencing and Neural Network Optimization and is Intel’s developer tool kit for machine learning inference. It contains a series of components and tools which help developers improve the inference performance of their models on Intel hardware, such as the AWS DeepRacer device, which has a compute module with an Intel Atom processor.

There are different kinds of compute hardware available today, such as CPUs and GPUs. You can think of these types of processors as different kinds of automobiles. The CPU might be similar to a family station wagon. It’s a great all-around car that’s good for different uses, but it probably won’t win a track race. Meanwhile, the GPU is like a purpose-built four-wheel drive car. It’s great for off-roading, but isn’t easy to drive around town. OpenVINO toolkit helps your optimize machine learning models to work on different types and combinations of hardware.

The Intel OpenVINO toolkit has the following components: the Deep Learning Deployment Toolkit (includes the model optimizer), inference engine, and pre-trained models. It also includes advanced tools and libraries for expert programmers.

Using an optimized model and the OpenVINO toolkit inference engine, we can achieve a low inference latency on the Intel Atom processor in the AWS DeepRacer device.

In the next section, we will go into detail about how the optimization process works.




There are three steps to the optimization process:

Convert and optimize
Further tuning for performance (optional)
Deploy the model
In the first step, we will run the model optimizer to prepare the pre-trained model for inferencing. It will generate a set of different files, known as an Intermediate Representation (IR), that the OpenVINO toolkit inference engine in AWS DeepRacer can understand and use. The model optimizer performs a number of optimizations on the model to make it run faster and more efficiently on the Intel Atom powered AWS DeepRacer. This is all done automatically when you export your model from the AWS DeepRacer Console, so that in most situations you’re done and can jump directly to step three and deploy the model to the AWS DeepRacer device.

However, in some cases, if the performance isn’t quite what you are expecting, there are plenty of additional tools included with the OpenVINO toolkit that advanced users can use. Think of these additional tools as you would modifying a car. Most drivers are happy with their cars as they come from the manufacturer. However, enthusiasts may wish to tune their cars or modify them in other ways to make them perform better. Investing the time to learn about and use these tools will allow you to get the best performance out of your AWS DeepRacer device.

Let’s take a look at a few of these tools.

The Post-Training Optimization Toolkit (POT) is a tool included with the OpenVINO toolkit, which can be used to make the model more efficient. Think of this tool as a slider with accuracy at one end and performance at the other.

The POT allows you to tune a model for extra performance by reducing the precision of the model and therefore, sacrificing some accuracy. Some Intel processors come with accelerators to turbo charge performance of lower precision models and this is where the POT is particularly handy, because it is similar to putting the right fuel in a car to make sure you get the best performance from the engine.

There are also benchmarking and accuracy checker tools available, which will allow you to get key performance and accuracy data for a model on the target hardware, such as AWS DeepRacer. This data is useful as you go through the process of refining and optimizing your model to understand how it impacts inference performance and accuracy.

When you are done with any of the optional fine-tuning, the optimized model is ready to be uploaded to the AWS DeepRacer device to perform real-time inference using the OpenVINO toolkit inference engine.